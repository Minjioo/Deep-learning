# Chapter 2
 
 ### 2.1 weight와 Bias의 이해
 Unit Step Function 활성화 함수를 사용한 인공 신경망
 > Bias: 민감도를 조절한는 파라미터
 > Weight: 각 자극의 중요도를 결정하는 파라미터
 > 인공 신경은 노드와 엣지로 이루어져 있고, 입력된 자극값에 weight를 곱하고 Bias를 더한 다음 활성화 함수를 통과시킴.   
 -> AI 가 적절한 Weight, Bias를 잘 찾아내도록!
      
 ### 2.2 인공 신경망과 MLP(multi-layer perceptron)
인공 신경망: 여러 인공 신경이 서로 연결되어.. 곱하고 더하고 액티베이션 하는 과정의 연속      
- 노드가 가진 웨이트 세트가 곧 해당 노드의 역할을 결정 -> 서로 충분히 달라야
> Input Layer
> Hidden Layer
> Output Layer

MLP: 모든 레이어가 FC 레이어인 신경망

### 2.3 인공 신경망은 함수다!
인공 신경망은 웨이트를 곱하고 바이어스를 더하고, 액티베이션을 통과시키는 작은 구성 요소들이 연결된 하나의 복잡한 함수.     
즉, **딥러닝은 거대한 AI 함수이며, 신경망이 가지는 웨이트와 바이어스를 찾아 최적의 함수를 찾는 것**이 목표!!

### 2.4 선형 회귀
> 회귀: 입력과 출력의 관계, 즉 입력과 출력을 연결하는 함수
> 선형 회귀: 이를 선으로 나타낸 것    

Loss 함수: 파라미터의 좋고 나쁨을 정량적으로 평가할 방법    
-> Loss를 최대한 줄여야 해!!
- MSE(Mean Squared Error): 제곱 사용 -> 파라미터 변화에 더 민감하게 반응 -> 이상치에 민감    
- MAE(Mean Absolute Error): 절대값 사용 -> 전체적인 데이터 트렌드에 집중    
      
a,b의 값을 바꿔가면서 Loss 함수를 그려, 최소의 Loss값을 찾는 것은 불가능, 파라미터가 너무 많음    
-> 경사하강법    
     
### 2.5 경사 하강법(Gradient Descent)
:임의로 a,b를 정한 후 그래디언트로 함숫값이 가장 가파르게 증가하는 방향을 찾고 그 반대 방향으로 이동,   
최소점에 도달할 때 까지 반복     
Learning rate(학습률): 그래디언트의 반대방향으로 이동할 때 보폭 조절.. 0.1, 0.001 ...      
       
단점: 계산 속도가 여전히 느림, Local Minimum에 빠질 수 있음      
      
-> 확률적 경사 하강법(Stochastic Gradient)       
        
### 2.6 웨이트 초기화
> Yann LeCun 
> Kaiming He
> Xavier
= 웨이트를 평균이 0인 랜덤한 값으로 초기화, 웨이트의 분산은 각 방식에 따라 다르게 설정






